/**
 * This work was created by participants in the DataONE project, and is
 * jointly copyrighted by participating institutions in DataONE. For 
 * more information on DataONE, see our web site at http://dataone.org.
 *
 *   Copyright ${year}
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *   http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and 
 * limitations under the License.
 * 
 * $Id$
 */

package org.dataone.cn.index.processor;

import java.io.File;
import java.util.HashMap;
import java.util.HashSet;
import java.util.LinkedList;
import java.util.List;
import java.util.Map;
import java.util.Queue;
import java.util.Set;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.ConcurrentSkipListSet;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.Future;
import java.util.concurrent.TimeUnit;
import java.util.concurrent.locks.Lock;
import java.util.concurrent.locks.ReentrantLock;

import org.apache.commons.collections4.queue.CircularFifoQueue;
import org.apache.commons.lang.StringUtils;
import org.apache.commons.lang3.ObjectUtils;
import org.apache.jena.atlas.logging.Log;
import org.apache.log4j.Logger;
import org.dataone.client.v2.formats.ObjectFormatCache;
import org.dataone.cn.hazelcast.HazelcastClientFactory;
import org.dataone.cn.index.task.IndexTask;
import org.dataone.cn.index.task.IndexTaskRepository;
import org.dataone.cn.index.task.ResourceMapIndexTask;
import org.dataone.cn.index.util.PerformanceLogger;
import org.dataone.cn.indexer.D1IndexerSolrClient;
import org.dataone.cn.indexer.XmlDocumentUtility;
import org.dataone.cn.indexer.parser.utility.SeriesIdResolver;
import org.dataone.cn.indexer.resourcemap.ForesiteResourceMap;
import org.dataone.cn.indexer.resourcemap.ResourceMap;
import org.dataone.cn.indexer.resourcemap.ResourceMapFactory;
import org.dataone.cn.indexer.solrhttp.SolrDoc;
import org.dataone.configuration.Settings;
import org.dataone.exceptions.MarshallingException;
import org.dataone.service.exceptions.BaseException;
import org.dataone.service.exceptions.NotFound;
import org.dataone.service.types.v1.Identifier;
import org.dataone.service.types.v1.ObjectFormatIdentifier;
import org.dataone.service.types.v1.TypeFactory;
import org.dataone.service.types.v2.ObjectFormat;
import org.dataone.service.types.v2.SystemMetadata;
import org.dspace.foresite.OREParserException;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.orm.ObjectOptimisticLockingFailureException;
import org.w3c.dom.Document;

/**
 * IndexTaskProcessor is the controller class for processing IndexTasks. These
 * tasks are generated by the IndexTaskGenerator class and associated
 * collaborators. IndexTaskProcessor uses the IndexTaskRepository to locate
 * IndexTasks for processing and delegates to IndexTaskProcessingStrategy
 * implementations for actual processing behavior.
 * 
 * @author sroseboo
 * 
 */
public class IndexTaskProcessor {

    
    
    private static Logger logger = Logger.getLogger(IndexTaskProcessor.class.getName());
    private static final String FORMAT_TYPE_DATA = "DATA";
    private static final String LOAD_LOGGER_NAME = "indexProcessorLoad";

    /* bean configuration causes static variables to be populated prior to Settings configurations
     * so things set with properties files need to be either refreshed in instance methods
     * or non-static.
     */

    private static final String THREADPOOL_SIZE_PROPERTY = "dataone.indexing.multiThreads.processThreadPoolSize";
    private static final int DEFAULT_THREADPOOL_SIZE = 5;
    private static final int DEFAULT_MAX_TRYCOUNT = 8;
    private static final int DEFAULT_MAXATTEMPTS = 10;
    
    //    private static int BATCH_UPDATE_SIZE = Settings.getConfiguration().getInt("dataone.indexing.batchUpdateSize", 1000);
    
    /* the executor service needs to be static, but static configuration is problematic when wanting to override it */
    private static int numProcessors = Settings.getConfiguration()
            .getInt(THREADPOOL_SIZE_PROPERTY, DEFAULT_THREADPOOL_SIZE);
    private static ExecutorService executor = Executors.newFixedThreadPool(numProcessors);

    private int maxAttempts = Settings.getConfiguration().getInt("dataone.indexing.multiThreads.resourceMapWait.maxAttempt", DEFAULT_MAXATTEMPTS);

    //    private static int FUTUREQUEUESIZE = Settings.getConfiguration().getInt("dataone.indexing.multiThreads.futureQueueSize", 100);


    private static final boolean CHECKING_ORE_READINESS = Settings.getConfiguration()
            .getBoolean("dataone.indexing.multiThreads.checkOreReadiness",false);
    
    private static final boolean DO_PRECHECKS_IN_MAIN_THREAD = Settings.getConfiguration()
                .getBoolean("dataone.indexing.multiThreads.taskPreCheckInMainThread",false);
    
    private static final boolean DELETE_OBSOLETED_AND_ARCHIVED = Settings.getConfiguration()
            .getBoolean("dataone.indexing.multiThreads.deleteObsoletedAndArchived",true);

    private static final boolean MARKING_IN_PROCESS = Settings.getConfiguration()
            .getBoolean("dataone.indexing.multiThreads.markingInProcess",false);
    
    private static final Lock LOCK = new ReentrantLock();
    
    /* a map used to aid in quick executor shutdown */
    private static Map<Future<Void>, IndexTask> futureMap = new HashMap<>();
    /* a queue used to aid in quick executor shutdown */
    private static List<Future<Void>> futureQueue = new LinkedList<>();
    /* a queue used to aid in quick executor shutdown, specifically to track tasks 
     * that are marked as in process, but not handed to the executor yet */
    private static Set<IndexTask> preSubmittedTasks = new HashSet<>();
    private static boolean inShutdownMode = false;
    
    //a concurrent map to maintain the information about current processing resource map objects and their referenced ids
    //the key is a referenced id and value is the id of resource map.
    private static ConcurrentHashMap <String, String> referencedIdsMap = new ConcurrentHashMap<String, String>(); 
    private static ConcurrentSkipListSet<String> seriesIdsSet = new ConcurrentSkipListSet<String>();
    
    @Autowired
    private IndexTaskRepository repo;

    @Autowired
    private IndexTaskProcessingStrategy deleteProcessor;

    @Autowired
    private IndexTaskProcessingStrategy updateProcessor;

    @Autowired
    private D1IndexerSolrClient d1IndexerSolrClient;

    @Autowired
    private String solrQueryUri;

    private PerformanceLogger perfLog = PerformanceLogger.getInstance();
    
    private static int maxTryCount = 8;
    
    public IndexTaskProcessor() {
        logger.warn("IndexTaskProcessor initialized with stated number of threads = " + numProcessors);
    }

    
    // TODO: figure out if this is used by index-build-tool, or if it's
    //  useful for the future.  It seems to be single-threaded, so out of date.
    /**
     * Processes the index task queue written by the IndexTaskGenerator, 
     * but unlike {@link #processIndexTaskQueue()}, all IndexTasks that 
     * add solr documents will be grouped into batches and done in one
     * command to solr.
     */
    /*public void batchProcessIndexTaskQueue() {
        logProcessorLoad();
        
        List<IndexTask> queue = getIndexTaskQueue();
        List<IndexTask> batchProcessList = new ArrayList<IndexTask>(BATCH_UPDATE_SIZE);

        logger.info("batchProcessIndexTaskQueue, queue size: " + queue.size() + " tasks");
        
        IndexTask nextTask = getNextIndexTask(queue);
        while (nextTask != null) {
            batchProcessList.add(nextTask);
            logger.info("added task: " + nextTask.getPid());
            nextTask = getNextIndexTask(queue);
            if (nextTask != null)
                logger.info("next task: " + nextTask.getPid());
            logger.info("queue size: " + queue.size());
            
            if (batchProcessList.size() >= BATCH_UPDATE_SIZE) {
                batchProcessTasksOnThread(batchProcessList);
                batchProcessList = new ArrayList<IndexTask>(BATCH_UPDATE_SIZE);
            }
        }
        batchProcessTasksOnThread(batchProcessList);
        
        List<IndexTask> retryQueue = getIndexTaskRetryQueue();
        List<IndexTask> batchProcessRetryList = new ArrayList<IndexTask>(BATCH_UPDATE_SIZE);
        
        logger.info("batchProcessIndexTaskQueue, retry queue size: " + queue.size() + " tasks");
        
        nextTask = getNextIndexTask(retryQueue);
        while (nextTask != null) {
            batchProcessRetryList.add(nextTask);
            nextTask = getNextIndexTask(retryQueue);
            
            if (batchProcessRetryList.size() >= BATCH_UPDATE_SIZE) {
                batchProcessTasksOnThread(batchProcessRetryList);
                batchProcessRetryList = new ArrayList<IndexTask>(BATCH_UPDATE_SIZE);
            }
        }
        batchProcessTasksOnThread(batchProcessRetryList);
    }*/
    
    /**
     * Start a round of IndexTask processing. The IndexTask data store is
     * abstracted as a queue of tasks to process ordered by priority and
     * modification date. Typically invoked periodically by a quartz scheduled
     * job.
     */
    public void processIndexTaskQueue() {
        logProcessorLoad();
        
 //       checkExecutorConfig();
        List<IndexTask> queue = getIndexTaskQueue();
        IndexTask task = getNextIndexTask(queue);
        while (task != null) {
            processTaskOnThread(task);
            task = getNextIndexTask(queue);
        }

        // effectively process failed tasks from previous run
        processFailedIndexTaskQueue();
        /*List<IndexTask> retryQueue = getIndexTaskRetryQueue();
        task = getNextIndexTask(retryQueue);
        while (task != null) {
            processTaskOnThread(task);
            task = getNextIndexTask(retryQueue);
        }*/
        
    }
    
    /**
     * Index the given index queue
     * @param queue
     */
    public void processIndexTaskQueue(List<IndexTask> queue) {
 //       checkExecutorConfig();
        IndexTask task = null;
        if(queue != null) {
            int size = queue.size();
            task = getNextIndexTask(queue);
            while (task != null) {
                processTaskOnThread(task);
                task = getNextIndexTask(queue);
            }
            logger.info("IndexTaskProcessor.processIndexTaskQueue - finish submitting the index task queue with the size "+size+ " and current queue size is down to "+queue.size());
        }
        
    }
    
    /**
     * Index the index task which currently has the failed status on the index task repository
     */
    public void processFailedIndexTaskQueue() {
        List<IndexTask> retryQueue = getIndexTaskRetryQueue();
        if(retryQueue != null) {
            IndexTask task = getNextIndexTask(retryQueue);
            logger.info("IndexTaskProcessor.processFailedIndexTaskQueue with size "+retryQueue.size());
            while (task != null) {
                processTaskOnThread(task);
                task = getNextIndexTask(retryQueue);
            }
        }
    }

    /**
     * Logs the number of {@link IndexTask}s that need to be processed
     * and the number of tasks that have failed.
     */
    private void logProcessorLoad() {
        
        Logger loadLogger = Logger.getLogger(LOAD_LOGGER_NAME);
        
        try {
            Long newTasks = repo.countByStatus(IndexTask.STATUS_NEW);
            Long failedTasks = repo.countByStatus(IndexTask.STATUS_FAILED);
            loadLogger.info("new tasks:" + newTasks + ", tasks previously failed: " + failedTasks );
            
        } catch (Exception e) {
            logger.error("Unable to count NEW or FAILED tasks in task index repository.", e);
        }
        
        
    }
    
    

    /*
     * Use multiple threads to process the index task
     */
    private void processTaskOnThread(final IndexTask task) {
        logger.info("using multiple threads to process index and the size of the thread pool is " + IndexTaskProcessor.numProcessors);
        
        Runnable newThreadTask = new Runnable() {
            public void run() {
                processTask(task);
            }
        };
        @SuppressWarnings("unchecked")
        Future<Void> future = (Future<Void>) getExecutorService().submit(newThreadTask);
        preSubmittedTasks.remove(task);
        futureQueue.add(future);
        futureMap.put(future, task);
    }
    
    public void processTask(IndexTask task) {
        
        if (task == null) {
            logger.debug("got sent a null task...ignoring");
            return;
        }
        
        long start = System.currentTimeMillis();
        try {
            if (CHECKING_ORE_READINESS) {
                checkReadinessProcessResourceMap(task);
            }
            if (DELETE_OBSOLETED_AND_ARCHIVED && task.isDeleteTask()) {
                logger.info("+++++++++++++start to process delete index task for "+task.getPid()+" in thread "+Thread.currentThread().getId());
                //System.out.println("+++++++++++++start to process delete index task for "+task.getPid()+" in thread "+Thread.currentThread().getId());
                deleteProcessor.process(task);
                //System.out.println("+++++++++++++end to process delete index task for "+task.getPid()+" in thread "+Thread.currentThread().getId());
                logger.info("+++++++++++++end to process delete index task for "+task.getPid()+" in thread "+Thread.currentThread().getId());
            } else {
                if (!DO_PRECHECKS_IN_MAIN_THREAD) {
                    task = doTaskPreChecks(task);
                    if (task == null) {
                        return;
                    }
                }
                logger.info("*********************start to process update index task for "+task.getPid()+" in thread "+Thread.currentThread().getId());
                //System.out.println("*********************start to process update index task for "+task.getPid()+" in thread "+Thread.currentThread().getId());
                updateProcessor.process(task);
                //System.out.println("*********************end to process update index task for "+task.getPid()+" in thread "+Thread.currentThread().getId());
                logger.info("*********************end to process update index task for "+task.getPid()+" in thread "+Thread.currentThread().getId());
            }
        } catch (InterruptedException interruptedE) {
            logger.warn("Task Interrupted before processing started. Resetting to NEW, for pid: " + task.getPid());
            task.markNew();
            repo.save(task);
            
        } catch (Exception e) {
            logger.error("Unable to process task for pid: " + task.getPid(), e);            
            repo.delete(task.getId());        
            handleFailedTask(task);
            return;
        } 
        finally {
            removeIdsFromResourceMapReferencedSetAndSeriesIdsSet(task);
        }
        repo.delete(task.getId());
   
        /*if(task != null && task instanceof ResourceMapIndexTask) {
            repo.delete(task.getId());//the ReousrceMapIndexTask is not the original object. repo.delete(IndexTask) wouldn't work.
        } else {
            repo.delete(task);
        }*/
        
        logger.info("Indexing complete for pid: " + task.getPid());
        if (perfLog.isLogEnabled())
        	perfLog.log("IndexTaskProcessor.processTasks process pid "+task.getPid(), System.currentTimeMillis()-start);
    }
    
    /*
     * When a resource map object is indexed, it will change the solr index of its referenced ids.
     * It can be a race condition. https://redmine.dataone.org/issues/7771
     * So we maintain a set containing the referenced ids which the resource map objects are currently being processed.
     * Before we start to process a new resource map object on a thread, we need to check the set.
     */
    private void checkReadinessProcessResourceMap(IndexTask task) throws InterruptedException, Exception{
        
        if (task == null) return;  // should be weeded out already...
        
        //only handle resourceMap index task
        long startTiming = System.currentTimeMillis();
        if(task != null && task instanceof ResourceMapIndexTask ) {
            
            if (logger.isDebugEnabled())
                logger.debug("$$$$$$$$$$$$$$$$$ the index task "+task.getPid()+" is a resource map task in the the thread "+ Thread.currentThread().getId());
            
            LOCK.lock();
            try {
            	if (perfLog.isLogEnabled())
            		perfLog.log("IndexTaskProcessor.checkReadiness/resMap/lock "+task.getPid(), System.currentTimeMillis()-startTiming);
            
                ResourceMapIndexTask resourceMapTask = (ResourceMapIndexTask) task;
                List<String> referencedIds = resourceMapTask.getReferencedIds();
                if(referencedIds != null) {
                    for (String id : referencedIds) {
                        if(SeriesIdResolver.isSeriesId(TypeFactory.buildIdentifier(id))) {
                            boolean isClear = false;
                            for(int i=0; i<maxAttempts; i++) {
                                if(seriesIdsSet.contains(id)) {
                                    System.out.println("###################Another index task is process the object with series id " 
                                            + id + " as well. So the thread to process id "
                                            + task.getPid() + " has to wait 0.5 seconds.");
                                    Thread.sleep(500);
                                } else {
                                    isClear = true;
                                    seriesIdsSet.add(id);
                                    break;
                                }
                            }
                            if(!isClear) {
                                removeIdsFromResourceMapReferencedSetAndSeriesIdsSet(task);
                                String message = "We waited for another thread to finish indexing a pid with series id "
                                + id + " for a while. Now we quit and can't index id "+task.getPid();
                                logger.error(message);
                                throw new Exception(message);
                            }
                            
                        }
                        
                        boolean clear = false;
                        for(int i=0; i<maxAttempts; i++) {
                            if(id != null && !id.trim().equals("") && referencedIdsMap.containsKey(id)) {
                                //another resource map is process the referenced id as well.
                                if(resourceMapTask.getPid().equals(referencedIdsMap.get(id))) {
                                    // this referenced id was put by the same resource map object. So we don't need wait.
                                    clear = true;
                                    break;
                                } else {
                                    // this referenced id was put by another resource map object. Wait .5 second.
                                    logger.info("###################Another resource map is process the referenced id "+id+" as well. So the thread to process id "
                                            +resourceMapTask.getPid()+" has to wait 0.5 seconds.");
                                    Thread.sleep(500);
                                }
                            } else if (id != null && !id.trim().equals("") && !referencedIdsMap.containsKey(id)) {
                                //no resource map is process the referenced id. It is good and we add it to the set.
                                referencedIdsMap.put(id, resourceMapTask.getPid());
                                clear = true;
                                break;
                            }
                        }
                        if(!clear) {
                            removeIdsFromResourceMapReferencedSetAndSeriesIdsSet(resourceMapTask);
                            String message = "We waited for another thread to finish indexing a resource map which has the referenced id "+id+
                                               " for a while. Now we quited and can't index id "+resourceMapTask.getPid();
                            logger.error(message);
                            throw new Exception(message);
                        }
                        
                    }
                }
                
            } catch (Exception e) {
                throw e;
            } finally {
                LOCK.unlock();
                if (perfLog.isLogEnabled()) {
                		perfLog.log("IndexTaskProcessor.checkReadiness/resMap process pid "+task.getPid(), System.currentTimeMillis()-startTiming);
            		}
            }
        } else {
            if (logger.isDebugEnabled())
                logger.debug("xxxxxxxxxxxxxxxxxxxx the index task "+task.getPid()
                        +" is NOT a resource map task in the the thread "+ Thread.currentThread().getId());
            
            Identifier pid = new Identifier();
            pid.setValue(task.getPid());
            SystemMetadata smd = HazelcastClientFactory.getSystemMetadataMap().get(pid);
            if(smd != null) {
                Identifier sid = smd.getSeriesId();
                if(sid != null && sid.getValue() != null && !sid.getValue().trim().equals("")) {
                    LOCK.lock();

                    long lockStart = System.currentTimeMillis();
              	      
                    try {
                    	if (perfLog.isLogEnabled()) 
                    		perfLog.log("IndexTaskProcessor.checkReadiness/other/lock "+task.getPid(), System.currentTimeMillis()-lockStart);
                        if (logger.isDebugEnabled())
                            logger.debug("xxxxxxxxxxxxxxxxxxxx the index task "+task.getPid()
                                    +" has a sid "+sid.getValue()+" in the the thread "+ Thread.currentThread().getId());
                        boolean clear = false;
                        for(int i=0; i<maxAttempts; i++) {
                            if(seriesIdsSet.contains(sid.getValue())) {
                                if (logger.isDebugEnabled())
                                    logger.debug("###################Another index task is process the object with series id "
                                            +sid.getValue()+" as well. So the thread to process id "
                                            +task.getPid()+" has to wait 0.5 seconds.");
                                Thread.sleep(500);
                            } else {
                                clear = true;
                                seriesIdsSet.add(sid.getValue());
                                break;
                            }
                        }
                        if(!clear) {
                            removeIdsFromResourceMapReferencedSetAndSeriesIdsSet(task);
                            String message = "We waited for another thread to finish indexing a pid with series id "+sid.getValue()+
                                               " for a while. Now we quited and can't index id "+task.getPid();
                            logger.error(message);
                            throw new Exception(message);
                        }
                    } catch (Exception e) {
                    	if (perfLog.isLogEnabled())
                    		perfLog.log("IndexTaskProcessor.checkReadiness/other/execption process pid "+task.getPid(), System.currentTimeMillis()-startTiming);
                    	throw e;  
                    } finally {
                        LOCK.unlock();
                    }
                }               
            }
            if (perfLog.isLogEnabled())
            	perfLog.log("IndexTaskProcessor.checkReadiness/other process pid "+task.getPid(), System.currentTimeMillis()-startTiming);
        }
    }
    
    
    /*
     * Remove the referenced ids from the set.
     */
    private void removeIdsFromResourceMapReferencedSetAndSeriesIdsSet(IndexTask task) {
        if(task != null && task instanceof ResourceMapIndexTask ) {
            ResourceMapIndexTask resourceMapTask = (ResourceMapIndexTask) task;
            List<String> referencedIds = resourceMapTask.getReferencedIds();
            if(referencedIds != null) {
                for (String id : referencedIds) {
                    if(id != null) {
                        referencedIdsMap.remove(id);
                        seriesIdsSet.remove(id);
                    }
                }
            }
        } else {
            Identifier id = new Identifier();
            id.setValue(task.getPid());
            SystemMetadata smd = HazelcastClientFactory.getSystemMetadataMap().get(id);
            logger.debug("remove the series id (if it has) for +++++ "+task.getPid());
            if(smd != null && smd.getSeriesId()!= null && smd.getSeriesId().getValue()!= null) {
                logger.debug("remove the series id "+smd.getSeriesId().getValue()+" for +++++ "+task.getPid());
                seriesIdsSet.remove(smd.getSeriesId().getValue());
            }
        }
    }
    
    /*
     * Use multiple threads to process the index task
     */
    /*private void batchProcessTasksOnThread(final List<IndexTask> taskList) {
        logger.info("using multiple threads to process BATCHED index tasks and the size of the pool is "+NUMOFPROCESSOR);
        Runnable newThreadTask = new Runnable() {
            public void run() {
                batchProcessTasks(taskList);
            }
        };
        Future future = executor.submit(newThreadTask);
        futureQueue.add(future);
    }*/

    /*private void batchProcessTasks(List<IndexTask> taskList) {
        if(taskList == null) {
            return;
        }
        long startBatch = System.currentTimeMillis();
        int size = taskList.size();
        logger.info("batch processing: " + size + " tasks");
        
        List<IndexTask> updateTasks = new ArrayList<>();
        List<IndexTask> deleteTasks = new ArrayList<>();
        
        for (IndexTask task : taskList) {
            if (task.isDeleteTask()) {
                logger.info("Adding delete task to be processed for pid: " + task.getPid());
                deleteTasks.add(task);
            } else {
                logger.info("Adding update task to be processed for pid: " + task.getPid());
                updateTasks.add(task);
            }
        }    
        
        logger.info("update tasks: " + updateTasks.size());
        logger.info("delete tasks: " + deleteTasks.size());
        try {
            batchCheckReadinessProcessResourceMap(taskList);
            try {
                deleteProcessor.process(deleteTasks);
                
                for (IndexTask task : deleteTasks) {
                    repo.delete(task);
                    logger.info("Indexing complete for pid: " + task.getPid());
                }
                
            } catch (Exception e) {
                StringBuilder failedPids = new StringBuilder(); 
                for (IndexTask task : deleteTasks)
                    failedPids.append(task.getPid()).append(", ");
                logger.error("Unable to process tasks for pids: " + failedPids.toString(), e);
                handleFailedTasks(deleteTasks);
            }
            
            try {
                updateProcessor.process(updateTasks);
                
                for (IndexTask task : updateTasks) {
                    repo.delete(task);
                    logger.info("Indexing complete for pid: " + task.getPid());
                }
                
            } catch (Exception e) {
                StringBuilder failedPids = new StringBuilder(); 
                for (IndexTask task : updateTasks)
                    failedPids.append(task.getPid()).append(", ");
                logger.error("Unable to process tasks for pids: " + failedPids.toString(), e);
                handleFailedTasks(deleteTasks);
            }
        } catch(Exception e) {
            logger.error("Couldn't batch indexing the tasks since "+e.getMessage());
        } finally {
            batchRemoveIdsFromResourceMapReferencedSet(taskList);
        }
        
        perfLog.log("IndexTaskProcessor.batchProcessTasks process "+size+" objects in ", System.currentTimeMillis()-startBatch);
    }*/
    
    private void batchCheckReadinessProcessResourceMap(List<IndexTask> tasks) throws Exception{
        LOCK.lock();
        try {
            if(tasks != null) {
                for (IndexTask task : tasks) {
                    checkReadinessProcessResourceMap(task);
                }
            }
            
        } finally {
            LOCK.unlock();
        }
    }
    
    private void batchRemoveIdsFromResourceMapReferencedSet(List<IndexTask> tasks) {
        if(tasks != null) {
            for (IndexTask task : tasks) {
                removeIdsFromResourceMapReferencedSetAndSeriesIdsSet(task);
            }
        }
    }
    
    private void handleFailedTasks(List<IndexTask> tasks) {
        for (IndexTask task : tasks) {
            task.markFailed();
            saveTaskWithoutDuplication(task);
        }
    }
    
    private void handleFailedTask(IndexTask task) {
        if(task != null) {
            task.markFailed();
            saveTaskWithoutDuplication(task);
        }
       
    }

    /**
     * returns the next ready task from the given queue. Readiness is determined
     * by availability of the object (unless it's a data object) 
     * 
     * @param queue
     * @return the next ready task
     */
    private IndexTask getNextIndexTask(List<IndexTask> queue) {
        IndexTask task = null;
        while (task == null && queue.isEmpty() == false && !inShutdownMode) {
            task = queue.remove(0);

            if (task == null) continue;
  
            if (DO_PRECHECKS_IN_MAIN_THREAD) {
                task = doTaskPreChecks(task);
            }
        }
        return task;
    }
          
    /*
     * return of null means don't proceed 
     * return of the task means proceed
     */
    private IndexTask doTaskPreChecks(IndexTask task) {

        if (MARKING_IN_PROCESS) {
            task.markInProgress();
            task = saveTask(task);
        }
        preSubmittedTasks.add(task);

        if (task == null) return null;  // saveTask can return null

        logger.info("Start of indexing pid: " + task.getPid());

        if (DELETE_OBSOLETED_AND_ARCHIVED) {
            try {       
                if (task.isDeleteTask()) 
                    return task;

            } catch (MarshallingException e ) {
                task.markFailed();
                saveTaskWithoutDuplication(task);
                logger.error(e.getMessage(),e);
                return null;
            }
        }

        if (!isObjectPathReady(task)) {
            if (MARKING_IN_PROCESS) {
                task.markNew();
                saveTaskWithoutDuplication(task);
            }
            logger.info("Task for pid: " + task.getPid() + " not processed since the object path is not ready.");
            return null;
        }

        if (CHECKING_ORE_READINESS) {
            if (representsResourceMap(task)) {
                boolean ready = true;               
                ResourceMap rm = null;
                List<String> referencedIds = null;
                try {
                    rm = ResourceMapFactory.buildResourceMap(task.getObjectPath());
                    referencedIds = rm.getAllDocumentIDs();

                    boolean found = referencedIds.remove(task.getPid());
                    // list.remove removes only the first occurrence found, so keep going until none left.
                    while(found) {
                        found = referencedIds.remove(task.getPid());
                    }

                    // if the resourceMap uses its SeriesId for the references, we need to remove it as well for this check.
                    // otherwise, it will never index.
                    SystemMetadata resMapSMD =  HazelcastClientFactory.getSystemMetadataMap().get(TypeFactory.buildIdentifier(task.getPid()));
                    if (resMapSMD.getSeriesId() != null) {
                        found = referencedIds.remove(resMapSMD.getSeriesId().getValue());
                        while(found) {
                            found = referencedIds.remove(resMapSMD.getSeriesId().getValue()); 
                        }
                    }


                    if (areAllReferencedDocsIndexed(referencedIds) == false) {
                        logger.info("****************Not all map resource references indexed for map: " + task.getPid()
                        + ".  Marking new and continuing...");
                        ready = false;
                    }
                } catch (OREParserException oreException) {
                    ready = false;
                    Throwable cause = oreException.getCause();
                    logger.error("Unable to parse ORE doc: " + task.getPid()
                    + ".  Unrecoverable parse error: task will not be re-tried.  Cause:: " 
                    + cause.getClass().getSimpleName() + ": " + cause.getMessage());
                    if (logger.isTraceEnabled()) {
                        oreException.printStackTrace();

                    }
                } catch (Exception e) {
                    ready = false;
                    logger.error("unable to load resource for pid: " + task.getPid()
                    + " at object path: " + task.getObjectPath()
                    + ".  Marking new and continuing...  Cause:: " + e.getClass().getSimpleName() + ": " + e.getMessage() );
                }
                if(!ready) {
                    task.markNew();
                    saveTaskWithoutDuplication(task);
                    logger.info("Task for resource map pid: " + task.getPid() + " not processed.");
                    return null;
                } else {
                    logger.info("the original index task - "+task.toString());
                    ResourceMapIndexTask resourceMapIndexTask = new ResourceMapIndexTask();
                    resourceMapIndexTask.copy(task);
                    resourceMapIndexTask.setReferencedIds(referencedIds);
                    task = resourceMapIndexTask;
                    if(task instanceof ResourceMapIndexTask) {
                        logger.info("the new index task is a ResourceMapIndexTask");
                        logger.info("the new index task - "+task.toString());
                    } else {
                        logger.error("Something is wrong to change the IndexTask object to the ResourceMapIndexTask object ");
                    }
                }
            }
        }
        return task;
    }


    /**
     * Referenced documents are PIDs or SIDs that are either archived or not.
     * This routine checks the solr index first, then failing to find a record,
     * checks HZ system metadata map.  
     * 
     * @param referencedIds
     * @return
     */
    private boolean areAllReferencedDocsIndexed(List<String> referencedIds) {
        if (referencedIds == null || referencedIds.size() == 0) {
            return true; // empty reference map...ok/ready to index.
        }
        List<SolrDoc> updateDocuments = null;
        int numberOfIndexedOrRemovedReferences = 0;
        try {
            updateDocuments = d1IndexerSolrClient.getDocumentsByD1Identifier(this.solrQueryUri, referencedIds);
            numberOfIndexedOrRemovedReferences = 0;
            for (String id : referencedIds) {
                boolean foundId = false;
                for (SolrDoc solrDoc : updateDocuments) {
                    if (solrDoc.getIdentifier().equals(id) || id.equals(solrDoc.getSeriesId())) {
                        foundId = true;
                        numberOfIndexedOrRemovedReferences++;
                        break;
                    }
                }
                if (foundId == false) {
                    // could be archived
                    Identifier pid = new Identifier();
                    pid.setValue(id);
                    logger.info("Identifier " + id
                            + " was not found in the referenced id list in the Solr search index.");
                    SystemMetadata smd = HazelcastClientFactory.getSystemMetadataMap().get(TypeFactory.buildIdentifier(id));
                    if (smd != null && notVisibleInIndex(smd)) {
                        numberOfIndexedOrRemovedReferences++;
                    }
                }
            }
        } catch (Exception e) {
            logger.error(e.getMessage(), e);
            return false;
        }
        return referencedIds.size() == numberOfIndexedOrRemovedReferences;
    }

    private boolean notVisibleInIndex(SystemMetadata smd) {
        
        if (smd == null) 
            return false;
            
        return ! SolrDoc.visibleInIndex(smd);
    }



    private boolean representsResourceMap(IndexTask task) {
        return ForesiteResourceMap.representsResourceMap(task.getFormatId());
    }

    /**
     * returns true if the task is for a data object, (the CN doesn't store these).
     * Otherwise, verify that the object path stored in the task, or the one in 
     * the Hazelcast object path map exists on the file system.
     * Updates the task with the one from Hazelcast if the Hazelcast one is valid,
     * and the one in the task is wrong or previously not set.

     * @return false if a valid path cannot be found
     */
    private boolean isObjectPathReady(IndexTask task) {

        if (isDataObject(task)) 
            return true;


        if (  !StringUtils.isBlank(task.getObjectPath())   
                && new File(task.getObjectPath()).exists() )
            return true;


        String hzObjectPath = retrieveHzObjectPath(task.getPid());
        if (hzObjectPath != null && new File(hzObjectPath).exists()) 
        {
            task.setObjectPath(hzObjectPath);
            return true;
        }

        // not ready if make it to here

        logger.info("Valid Object Path could not be found for pid: " + task.getPid()
                + "  Checked path strings in the task [" + task.getObjectPath() + 
                "] and Hazelcast objectPathMap [" + hzObjectPath + 
                "]");

        return false;
    }
    

    private boolean isDataObject(IndexTask task) {
        ObjectFormat format = null;
        try {
            format = ObjectFormatCache.getInstance().getFormat(TypeFactory.buildFormatIdentifier(task.getFormatId()));
        } catch (NotFound e) {
            logger.warn(String.format("object format for '%s' with formatid '%s' could not be found!!",
                    task.getPid(), 
                    task.getFormatId()));
            return false;
        }
        return FORMAT_TYPE_DATA.equals(format.getFormatType());
    }

    private String retrieveHzObjectPath(String pidString) {
        Identifier pid = TypeFactory.buildIdentifier(pidString);
        String path = HazelcastClientFactory.getObjectPathMap().get(pid);
        if (path == null) {
            // cleanup the map
            HazelcastClientFactory.getObjectPathMap().evict(pid);
            if (logger.isDebugEnabled())
                logger.debug("did not find Object Path for pid: " + pidString
                        + " cleaning up the map by evicting the pid.");
        }
        return path;
    }

    public List<IndexTask> getIndexTaskQueue() {
        
        maxTryCount = Settings.getConfiguration().getInt("dataone.indexing.processing.max.tryCount", DEFAULT_MAX_TRYCOUNT);
        long getIndexTasksStart = System.currentTimeMillis();
        logger.info("New index tasks with less than "+maxTryCount+" try-count (resource maps sometimes will be set the status new even though the indexing failed) in the index queue will be processed.");
        //List<IndexTask> indexTasks = repo.findByStatusOrderByPriorityAscTaskModifiedDateAsc(IndexTask.STATUS_NEW);
        List<IndexTask> indexTasks = repo.findByStatusAndTryCountLessThanOrderByPriorityAscTaskModifiedDateAsc(IndexTask.STATUS_NEW, maxTryCount);
        if (perfLog.isLogEnabled())
        	perfLog.log("IndexTaskProcessor.getIndexTaskQueue() fetching NEW IndexTasks from repo", System.currentTimeMillis() - getIndexTasksStart);
        return indexTasks;
    }

    private List<IndexTask> getIndexTaskRetryQueue() {
        //return repo.findByStatusAndNextExecutionLessThan(IndexTask.STATUS_FAILED, System.currentTimeMillis());
        logger.info("Failed index tasks with less than "+maxTryCount+" try-count in the index queue will be processed.");
        return repo.findByStatusAndNextExecutionLessThanAndTryCountLessThan(IndexTask.STATUS_FAILED, System.currentTimeMillis(), maxTryCount);
    }

    /*
     * @return - Can return null upon error
     */
    private IndexTask saveTask(IndexTask task) {
        try {
            task = repo.save(task);
            logger.info("IndexTaskProcess.saveTask save the index task "+task.getPid());
        } catch (ObjectOptimisticLockingFailureException e) {
            logger.error("Unable to update index task for pid: " + task.getPid() + ".");
            task = null;
        }
        return task;
    }
    
    /*
     * Save the task only if no new or failed task already exists for the pid
     */
    private void saveTaskWithoutDuplication(IndexTask task) {
        if(task != null) {
            if(!newOrFailedIndexTaskExists(task.getPid())) {
                    saveTask(task);
            }
        }
    }
    
    /**
     * If an index task exists with the new or failed status for the given id
     * @param id
     * @return true if the index task with new or failed status exists; otherwise false.
     */
    private boolean newOrFailedIndexTaskExists(String id) {
        logger.info("IndexTaskProcess.newOrFailedIndexTaskExists for id "+id);
        boolean exist=false;
        if(id != null ) {
            List<IndexTask> itList = repo.findByPidAndStatus(id, IndexTask.STATUS_NEW);
            if(itList != null && !itList.isEmpty()) {
                logger.info("IndexTaskProcess.newOrFailedIndexTaskExists did find a new-status index task for id "+id);
                exist = true;
            }
            if(!exist) {
                itList = repo.findByPidAndStatus(id, IndexTask.STATUS_FAILED);
                if(itList != null && !itList.isEmpty()) {
                    logger.info("IndexTaskProcess.newOrFailedIndexTaskExists did find a failed-status index task for id "+id);
                    exist = true;
                }
            }
        }
        
        return exist;
    }

    private Document loadDocument(IndexTask task) {
        Document docObject = null;
        try {
            docObject = XmlDocumentUtility.loadDocument(task.getObjectPath());
        } catch (Exception e) {
            logger.error(e.getMessage(), e);
        }
        if (docObject == null) {
            logger.error("Could not load OBJECT file for ID,Path=" + task.getPid() + ", "
                    + task.getObjectPath());
        }
        return docObject;
    }

    public void setSolrQueryUri(String uri) {
        this.solrQueryUri = uri;
    }
    
    /**
     * gets and possibly resets the executor service
     * @return
     */
    public ExecutorService getExecutorService() {
//        int poolSizeFromConfig = Settings.getConfiguration()
//                .getInt(THREADPOOL_SIZE_PROPERTY, DEFAULT_THREADPOOL_SIZE);
//        logger.warn("read number of threads '" + THREADPOOL_SIZE_PROPERTY + "' with value: " + poolSizeFromConfig);
//        
//        if (poolSizeFromConfig != numProcessors) {
//            // the configuration has changed. log and configure new executor
//                      
//            // if there was an existing executor, shut it down
//            if (executor != null) {
//                shutdownExecutor();
//                executor = null; // 
//            }
//            
//            logger.warn("!!!!!! A new FixedThreadPool executor created.  Num threads = " + poolSizeFromConfig);
//            
//            executor = Executors.newFixedThreadPool(poolSizeFromConfig);
//            
//            // remember the (new) current pool size for later
//            numProcessors = poolSizeFromConfig;
//            
//            logger.warn(" -- !! New poolsize set to: " + numProcessors);
//        }
//        if (executor == null) {
//            // will probably only be called once... 
//            logger.warn("!!!!!! Initial creation of executor: A new FixedThreadPool executor created.  Num threads = " + numProcessors);
//            executor = Executors.newFixedThreadPool(numProcessors);
//        }
       return executor;
    }
    
    /**
     * Get the list of index task futures submitted to the executor service.
     * @return
     */
    public Queue<Future<Void>> getFutureQueue() {
        return new CircularFifoQueue<Future<Void>>(futureQueue);
    }
    
    
//    private void checkExecutorConfig() {
//        int poolSizeFromConfig = Settings.getConfiguration()
//                .getInt("dataone.indexing.multiThreads.processThreadPoolSize", DEFAULT_THREADPOOL_SIZE);
//        
//        // 
//        if (poolSizeFromConfig != numProcessors) {
//            // the configuration has changed. log and configure new executor
//            if (executor != null) {
//                shutdownExecutor(); 
//            }       
//            logger.warn("!!!!!! A new FixedThreadPool executor created.  Num threads = " + poolSizeFromConfig);
//            
//            executor = Executors.newFixedThreadPool(poolSizeFromConfig);
//            
//            numProcessors = poolSizeFromConfig;
//            logger.warn(" -- !! New poolsize set to: " + numProcessors);
//        }
//        
//    }
    
    
    /**
     * Call at the end of processing, when canceled tasks would otherwise be left 
     * in the IN PROCESS state.
     */
    public void shutdownExecutor() {
        
        inShutdownMode = true;
        
        logger.warn("processor [" + this + "] Shutting down the ExecutorService.  Will allow active tasks to finish; " +
        		"will cancel submitted tasks and return them to NEW status, wait for active tasks to finish, then " +
        		"return any remaining task not yet submitted to NEW status....");
        logger.warn("...1.) closing ExecutorService to new tasks...");
        getExecutorService().shutdown();
        
        // now, no tasks can be added to the ExecutorService's internal queue
        // some tasks are active and can't be canceled, some are waiting in this
        // internal queue and can be canceled (in theory)
        logger.warn("...2.) canceling cancelable futures...");
        logger.warn(String.format("...number of futures: %d", futureQueue.size()));
        logger.warn("... number of tasks in futures map: " + futureMap.size());
        int marked = 0;
        int noTaskMapping = 0;
        List<Future<Void>> uncancelable = new LinkedList<>();
        // cycle through list of futures to see which are cancelable
        if (!futureQueue.isEmpty()) {
            for (int i=futureQueue.size()-1; i > -1; i--) {
                Future<Void> f = futureQueue.get(i);
                if (f.cancel(/* interrupt while running? */ false)) {
                    // cancelable, so try to mark new
                    IndexTask t = futureMap.get(f);
                    if (t != null) {
                        try {
                            t.setStatus(IndexTask.STATUS_NEW);
                            repo.save(t);
                            marked++; 
                            logger.warn("IndexTaskProcessor.shutdownExecutor - task returned to NEW status for object " + t.getPid());
                        } catch (Exception e) {
                            logger.error("IndexTaskProcessor.shutdownExecutor - task canceled for object " + t.getPid() + " but could not be returned to NEW status. Exception raised: "
                                    + e.getClass().getCanonicalName() + ": " + e.getMessage(), e);
                        }
                    } else {
                        noTaskMapping++;
                    }
                } else {
                    uncancelable.add(f);
                    // uncancelables includes completed, previously cancelled tasks, 
                    // and "others" that for some reason could not be cancelled.
                }
            }
            logger.warn(String.format("...number of (cancelable) runnables/tasks reset to new: %d", marked));
            logger.warn(String.format("...number of (cancelable) runnables not mapped to tasks: %d", noTaskMapping));
            logger.warn(String.format("...number of uncancelable runnables: %d (completed or in process)", uncancelable.size()));
        } 
        try {
            logger.warn("...3.) waiting (with timeout) for active futures to finish...");
            // the typical time for the longest type of task is 2 minutes (for ResourceMaps)
            // so wait a little longer then give up
            getExecutorService().awaitTermination(3, TimeUnit.MINUTES);
            
            // we should probably report on the in-process threads
            logger.warn("...4.) Reviewing remaining uncancellables to check for completion, returning incomplete ones to NEW status...");
            if (!uncancelable.isEmpty()) {
                for (Future<Void> f : uncancelable) {
                    // TODO: this block is never executed because after calling f.cancel(), 
                    // f.isDone() is always true, according to f.cancel() documentation
                    if (!f.isDone()) {
                        IndexTask t = futureMap.get(f);
                        if (t != null) {
                            if (IndexTask.STATUS_IN_PROCESS.equals(t.getStatus())) {
                                try {
                                    t.markNew();
                                    repo.save(t);
                                    logger.warn("...active future for pid " + t.getPid() 
                                        + " not done.  Resetting to NEW, to allow reprocessing next time...");
                                } catch (Exception e) {
                                    logger.warn("IndexTaskProcessor.shutdownExecutor - can't reset the status to new and save the index task for the object "+t.getPid()+ " since "+e.getMessage(), e);
                                }
                            } else {
                                logger.warn("...active future for pid " + t.getPid() + 
                                        "completed during wait period with status " + t.getStatus());
                            }
                        } else {
                            logger.error("...CANNOT requeue task. No task mapped to this future!!");
                        }
                    }
                }
            }
        } catch (InterruptedException e) {
            logger.warn("interrupt caught while waiting for executor service to finish executing uninterruptable tasks.");
        } finally {
            logger.warn("...5.) Calling shutdownNow on the executor service.");
            List<Runnable> stillWaiting = getExecutorService().shutdownNow();
            logger.warn("... .... number of runnables still waiting: " + stillWaiting.size());

            logger.warn("...6.) returning preSubmitted tasks to NEW status...");
            logger.warn("... .... number of preSubmitted tasks: " + preSubmittedTasks.size());
            for(IndexTask t : preSubmittedTasks) {
                try {
                    t.markNew();
                    repo.save(t);
                    logger.warn("... preSubmittedTask for pid " + t.getPid() + "returned to NEW status.");
                } catch (Throwable e) {
                    logger.error("....... Exception thrown trying to return task to NEW status for pid: " + t.getPid(),e);
                }
            }
            logger.warn("............7.) DONE with shutting down IndexTaskProcessor.");
        }
    }
}
